{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(name, shape):\n",
    "    return tf.get_variable(name, shape, initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    return tf.get_variable(name, shape, initializer = tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic One-Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.14\n",
      "step 100, train accuracy 0.78\n",
      "step 200, train accuracy 0.9\n",
      "step 300, train accuracy 0.88\n",
      "step 400, train accuracy 0.84\n",
      "step 500, train accuracy 0.91\n",
      "step 600, train accuracy 0.83\n",
      "step 700, train accuracy 0.9\n",
      "step 800, train accuracy 0.81\n",
      "step 900, train accuracy 0.95\n",
      "test accuracy 0.8861\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "w1 = weight_variable('w1', [784,10])\n",
    "b1 = bias_variable('b1', [10])\n",
    "\n",
    "z1 = tf.matmul(x, w1) + b1\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z1))\n",
    "train_step = tf.train.AdamOptimizer(0.5).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(z1,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(1000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1]})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Two-Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.1\n",
      "step 100, train accuracy 0.82\n",
      "step 200, train accuracy 0.88\n",
      "step 300, train accuracy 0.91\n",
      "step 400, train accuracy 0.93\n",
      "step 500, train accuracy 0.92\n",
      "step 600, train accuracy 0.81\n",
      "step 700, train accuracy 0.92\n",
      "step 800, train accuracy 0.91\n",
      "step 900, train accuracy 0.93\n",
      "test accuracy 0.9306\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "w1 = weight_variable('w1', [784,20])\n",
    "b1 = bias_variable('b1', [20])\n",
    "\n",
    "z1 = tf.matmul(x, w1) + b1\n",
    "a1 = tf.nn.relu(z1)\n",
    "\n",
    "w2 = weight_variable('w2', [20,10])\n",
    "b2 = bias_variable('b2', [10])\n",
    "\n",
    "z2 = tf.matmul(a1, w2) + b2\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z2))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(z2,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(1000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1]})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Two-Layer NN using tf.contrib.learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_master': '', '_task_type': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_task_id': 0, '_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd5b526c0b8>, '_keep_checkpoint_every_n_hours': 10000, '_environment': 'local', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_evaluation_master': ''}\n",
      "WARNING:tensorflow:From /home/udocker/phoenix-worker/environments/python3/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into tmp/mnist_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.178382, step = 1001\n",
      "INFO:tensorflow:global_step/sec: 228.138\n",
      "INFO:tensorflow:loss = 0.259748, step = 1101\n",
      "INFO:tensorflow:global_step/sec: 211.846\n",
      "INFO:tensorflow:loss = 0.272732, step = 1201\n",
      "INFO:tensorflow:global_step/sec: 246.841\n",
      "INFO:tensorflow:loss = 0.250179, step = 1301\n",
      "INFO:tensorflow:global_step/sec: 241.096\n",
      "INFO:tensorflow:loss = 0.227868, step = 1401\n",
      "INFO:tensorflow:global_step/sec: 208.18\n",
      "INFO:tensorflow:loss = 0.266438, step = 1501\n",
      "INFO:tensorflow:global_step/sec: 224.486\n",
      "INFO:tensorflow:loss = 0.169383, step = 1601\n",
      "INFO:tensorflow:global_step/sec: 202.933\n",
      "INFO:tensorflow:loss = 0.171079, step = 1701\n",
      "INFO:tensorflow:global_step/sec: 208.959\n",
      "INFO:tensorflow:loss = 0.323249, step = 1801\n",
      "INFO:tensorflow:global_step/sec: 211.45\n",
      "INFO:tensorflow:loss = 0.306178, step = 1901\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into tmp/mnist_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.175457.\n",
      "WARNING:tensorflow:From /home/udocker/phoenix-worker/environments/python3/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-01-16:57:46\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-01-16:57:51\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9393, auc = 0.994543, global_step = 2000, loss = 0.219725\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "\n",
      "Test Accuracy: 0.939300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Specify that all features have real-value data\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=784)]\n",
    "\n",
    "classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n",
    "                                              hidden_units=[20, 10],\n",
    "                                              n_classes=10,\n",
    "                                              optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "                                              activation_fn = tf.nn.relu,\n",
    "                                              model_dir=\"tmp/mnist_model\")\n",
    "\n",
    "def generate_input_fn(data, label):\t\n",
    "    image_batch, label_batch = tf.train.shuffle_batch(\n",
    "            [data, label]\n",
    "            , batch_size=100\n",
    "            , capacity=800\n",
    "            , min_after_dequeue=400\n",
    "            , enqueue_many=True)\n",
    "    return image_batch, label_batch\n",
    "\n",
    "def input_fn_for_train():\n",
    "    train_data = tf.constant(np.array(mnist.train.images, 'float32'))\n",
    "    train_target = tf.argmax(tf.constant(np.array(mnist.train.labels, 'int64')), axis=1)\n",
    "    return generate_input_fn(train_data, train_target)\n",
    "\n",
    "# Fit model.\n",
    "classifier.fit(input_fn=input_fn_for_train, steps=1000)\n",
    "\n",
    "# Define the test inputs\n",
    "def get_test_inputs():\n",
    "    x = tf.constant(np.array(mnist.test.images, 'float32'))\n",
    "    y = tf.argmax(tf.constant(np.array(mnist.test.labels, 'int64')), axis=1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(input_fn=get_test_inputs,\n",
    "                                       steps=1)[\"accuracy\"]\n",
    "\n",
    "print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Three-Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.07\n",
      "step 100, train accuracy 0.91\n",
      "step 200, train accuracy 0.91\n",
      "step 300, train accuracy 0.95\n",
      "step 400, train accuracy 0.87\n",
      "step 500, train accuracy 0.94\n",
      "step 600, train accuracy 0.91\n",
      "step 700, train accuracy 0.89\n",
      "step 800, train accuracy 0.96\n",
      "step 900, train accuracy 0.92\n",
      "test accuracy 0.9523\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "w1 = weight_variable('w1', [784,50])\n",
    "b1 = bias_variable('b1', [50])\n",
    "\n",
    "z1 = tf.matmul(x, w1) + b1\n",
    "a1 = tf.nn.relu(z1)\n",
    "\n",
    "w2 = weight_variable('w2', [50,25])\n",
    "b2 = bias_variable('b2', [25])\n",
    "\n",
    "z2 = tf.matmul(a1, w2) + b2\n",
    "a2 = tf.nn.relu(z2)\n",
    "\n",
    "w3 = weight_variable('w3', [25,10])\n",
    "b3 = bias_variable('b3', [10])\n",
    "\n",
    "z3 = tf.matmul(a2, w3) + b3\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z3))\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(z3,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(1000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1]})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Three-Layer NN with BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.07\n",
      "step 100, train accuracy 0.85\n",
      "step 200, train accuracy 0.9\n",
      "step 300, train accuracy 0.98\n",
      "step 400, train accuracy 0.95\n",
      "step 500, train accuracy 0.92\n",
      "step 600, train accuracy 0.93\n",
      "step 700, train accuracy 0.92\n",
      "step 800, train accuracy 0.97\n",
      "step 900, train accuracy 0.94\n",
      "step 1000, train accuracy 0.93\n",
      "step 1100, train accuracy 0.97\n",
      "step 1200, train accuracy 0.96\n",
      "step 1300, train accuracy 0.95\n",
      "step 1400, train accuracy 0.94\n",
      "step 1500, train accuracy 0.94\n",
      "step 1600, train accuracy 0.93\n",
      "step 1700, train accuracy 0.94\n",
      "step 1800, train accuracy 0.95\n",
      "step 1900, train accuracy 0.92\n",
      "test accuracy 0.969\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "w1 = weight_variable('w1', [784,50])\n",
    "b1 = bias_variable('b1', [50])\n",
    "\n",
    "z1 = tf.matmul(x, w1) + b1\n",
    "zbn1 = tf.layers.batch_normalization(z1, training = phase)\n",
    "a1 = tf.nn.relu(zbn1)\n",
    "\n",
    "w2 = weight_variable('w2', [50, 25])\n",
    "b2 = bias_variable('b2', [25])\n",
    "\n",
    "z2 = tf.matmul(a1, w2) + b2\n",
    "zbn2 = tf.layers.batch_normalization(z2, training = phase)\n",
    "a2 = tf.nn.relu(zbn2)\n",
    "\n",
    "w3 = weight_variable('w3', [25, 10])\n",
    "b3 = bias_variable('b3', [10])\n",
    "\n",
    "z3 = tf.matmul(a2, w3) + b3\n",
    "zbn3 = tf.layers.batch_normalization(z3, training = phase)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = zbn3))\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(zbn3,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(2000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], phase:True})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run([train_step, update_ops], feed_dict={x: batch[0], y_: batch[1], phase: True})\n",
    "    print(\"test accuracy %s\"% (accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, phase: False})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Three-Layer NN with BatchNorm and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.14\n",
      "step 100, train accuracy 0.79\n",
      "step 200, train accuracy 0.82\n",
      "step 300, train accuracy 0.86\n",
      "step 400, train accuracy 0.89\n",
      "step 500, train accuracy 0.9\n",
      "step 600, train accuracy 0.85\n",
      "step 700, train accuracy 0.88\n",
      "step 800, train accuracy 0.84\n",
      "step 900, train accuracy 0.9\n",
      "step 1000, train accuracy 0.9\n",
      "step 1100, train accuracy 0.86\n",
      "step 1200, train accuracy 0.86\n",
      "step 1300, train accuracy 0.92\n",
      "step 1400, train accuracy 0.91\n",
      "step 1500, train accuracy 0.91\n",
      "step 1600, train accuracy 0.89\n",
      "step 1700, train accuracy 0.92\n",
      "step 1800, train accuracy 0.88\n",
      "step 1900, train accuracy 0.89\n",
      "step 2000, train accuracy 0.89\n",
      "step 2100, train accuracy 0.91\n",
      "step 2200, train accuracy 0.86\n",
      "step 2300, train accuracy 0.88\n",
      "step 2400, train accuracy 0.94\n",
      "step 2500, train accuracy 0.89\n",
      "step 2600, train accuracy 0.92\n",
      "step 2700, train accuracy 0.9\n",
      "step 2800, train accuracy 0.95\n",
      "step 2900, train accuracy 0.97\n",
      "step 3000, train accuracy 0.91\n",
      "step 3100, train accuracy 0.86\n",
      "step 3200, train accuracy 0.94\n",
      "step 3300, train accuracy 0.94\n",
      "step 3400, train accuracy 0.92\n",
      "step 3500, train accuracy 0.88\n",
      "step 3600, train accuracy 0.93\n",
      "step 3700, train accuracy 0.92\n",
      "step 3800, train accuracy 0.98\n",
      "step 3900, train accuracy 0.91\n",
      "step 4000, train accuracy 0.91\n",
      "step 4100, train accuracy 0.91\n",
      "step 4200, train accuracy 0.99\n",
      "step 4300, train accuracy 0.97\n",
      "step 4400, train accuracy 0.93\n",
      "step 4500, train accuracy 0.93\n",
      "step 4600, train accuracy 0.93\n",
      "step 4700, train accuracy 0.96\n",
      "step 4800, train accuracy 0.93\n",
      "step 4900, train accuracy 0.9\n",
      "test accuracy 0.9715\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = weight_variable('w1', [784,50])\n",
    "b1 = bias_variable('b1', [50])\n",
    "\n",
    "z1 = tf.matmul(x, w1) + b1\n",
    "zbn1 = tf.layers.batch_normalization(z1, training = phase)\n",
    "a1 = tf.nn.relu(zbn1)\n",
    "\n",
    "w2 = weight_variable('w2', [50, 25])\n",
    "b2 = bias_variable('b2', [25])\n",
    "\n",
    "z2 = tf.matmul(a1, w2) + b2\n",
    "zbn2 = tf.layers.batch_normalization(z2, training = phase)\n",
    "a2 = tf.nn.relu(zbn2)\n",
    "h_fc2_drop = tf.nn.dropout(a2, keep_prob)\n",
    "\n",
    "\n",
    "w3 = weight_variable('w3', [25, 10])\n",
    "b3 = bias_variable('b3', [10])\n",
    "\n",
    "z3 = tf.matmul(h_fc2_drop, w3) + b3\n",
    "zbn3 = tf.layers.batch_normalization(z3, training = phase)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = zbn3))\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(zbn3,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(5000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], phase:True, keep_prob: 0.5})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run([train_step, update_ops], feed_dict={x: batch[0], y_: batch[1], phase: True, keep_prob: 0.5})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, phase: False, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Four-Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.09\n",
      "step 100, train accuracy 0.96\n",
      "step 200, train accuracy 0.94\n",
      "step 300, train accuracy 0.89\n",
      "step 400, train accuracy 1\n",
      "step 500, train accuracy 0.96\n",
      "step 600, train accuracy 0.97\n",
      "step 700, train accuracy 0.92\n",
      "step 800, train accuracy 0.99\n",
      "step 900, train accuracy 0.97\n",
      "test accuracy 0.9588\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "w1 = weight_variable('w1', [784,128])\n",
    "b1 = bias_variable('b1', [128])\n",
    "\n",
    "z1 = tf.matmul(x, w1) + b1\n",
    "a1 = tf.nn.relu(z1)\n",
    "\n",
    "w2 = weight_variable('w2', [128, 64])\n",
    "b2 = bias_variable('b2', [64])\n",
    "\n",
    "z2 = tf.matmul(a1, w2) + b2\n",
    "a2 = tf.nn.relu(z2)\n",
    "\n",
    "w3 = weight_variable('w3', [64, 32])\n",
    "b3 = bias_variable('b3', [32])\n",
    "\n",
    "z3 = tf.matmul(a2, w3) + b3\n",
    "a3 = tf.nn.relu(z3)\n",
    "\n",
    "w4 = weight_variable('w4', [32, 10])\n",
    "b4 = bias_variable('b4', [10])\n",
    "\n",
    "z4 = tf.matmul(a3, w4) + b4\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z4))\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(z4,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(1000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1]})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Four-Layer NN with Dropout in Last Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.1\n",
      "step 100, train accuracy 0.77\n",
      "step 200, train accuracy 0.8\n",
      "step 300, train accuracy 0.89\n",
      "step 400, train accuracy 0.89\n",
      "step 500, train accuracy 0.89\n",
      "step 600, train accuracy 0.93\n",
      "step 700, train accuracy 0.94\n",
      "step 800, train accuracy 0.95\n",
      "step 900, train accuracy 0.94\n",
      "step 1000, train accuracy 0.86\n",
      "step 1100, train accuracy 0.97\n",
      "step 1200, train accuracy 0.92\n",
      "step 1300, train accuracy 0.97\n",
      "step 1400, train accuracy 0.94\n",
      "step 1500, train accuracy 0.94\n",
      "step 1600, train accuracy 0.92\n",
      "step 1700, train accuracy 0.95\n",
      "step 1800, train accuracy 0.94\n",
      "step 1900, train accuracy 0.95\n",
      "step 2000, train accuracy 0.96\n",
      "step 2100, train accuracy 0.96\n",
      "step 2200, train accuracy 0.93\n",
      "step 2300, train accuracy 0.96\n",
      "step 2400, train accuracy 0.94\n",
      "step 2500, train accuracy 0.92\n",
      "step 2600, train accuracy 0.95\n",
      "step 2700, train accuracy 0.96\n",
      "step 2800, train accuracy 0.98\n",
      "step 2900, train accuracy 0.96\n",
      "step 3000, train accuracy 0.99\n",
      "step 3100, train accuracy 0.96\n",
      "step 3200, train accuracy 0.96\n",
      "step 3300, train accuracy 0.99\n",
      "step 3400, train accuracy 0.98\n",
      "step 3500, train accuracy 0.97\n",
      "step 3600, train accuracy 0.97\n",
      "step 3700, train accuracy 0.96\n",
      "step 3800, train accuracy 0.97\n",
      "step 3900, train accuracy 0.96\n",
      "step 4000, train accuracy 0.97\n",
      "step 4100, train accuracy 0.97\n",
      "step 4200, train accuracy 0.98\n",
      "step 4300, train accuracy 0.97\n",
      "step 4400, train accuracy 1\n",
      "step 4500, train accuracy 0.95\n",
      "step 4600, train accuracy 0.94\n",
      "step 4700, train accuracy 0.96\n",
      "step 4800, train accuracy 0.97\n",
      "step 4900, train accuracy 0.97\n",
      "test accuracy 0.9766\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = weight_variable('w1', [784,128])\n",
    "b1 = bias_variable('b1', [128])\n",
    "\n",
    "z1 = tf.matmul(x, w1) + b1\n",
    "a1 = tf.nn.relu(z1)\n",
    "\n",
    "w2 = weight_variable('w2', [128, 64])\n",
    "b2 = bias_variable('b2', [64])\n",
    "\n",
    "z2 = tf.matmul(a1, w2) + b2\n",
    "a2 = tf.nn.relu(z2)\n",
    "\n",
    "w3 = weight_variable('w3', [64, 32])\n",
    "b3 = bias_variable('b3', [32])\n",
    "\n",
    "z3 = tf.matmul(a2, w3) + b3\n",
    "a3 = tf.nn.relu(z3)\n",
    "h_fc3_drop = tf.nn.dropout(a3, keep_prob)\n",
    "\n",
    "w4 = weight_variable('w4', [32, 10])\n",
    "b4 = bias_variable('b4', [10])\n",
    "\n",
    "z4 = tf.matmul(h_fc3_drop, w4) + b4\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z4))\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(z4,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(5000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Four-Layer NN with Dropout in All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.13\n",
      "step 100, train accuracy 0.62\n",
      "step 200, train accuracy 0.65\n",
      "step 300, train accuracy 0.69\n",
      "step 400, train accuracy 0.8\n",
      "step 500, train accuracy 0.75\n",
      "step 600, train accuracy 0.73\n",
      "step 700, train accuracy 0.83\n",
      "step 800, train accuracy 0.76\n",
      "step 900, train accuracy 0.82\n",
      "step 1000, train accuracy 0.87\n",
      "step 1100, train accuracy 0.79\n",
      "step 1200, train accuracy 0.75\n",
      "step 1300, train accuracy 0.8\n",
      "step 1400, train accuracy 0.8\n",
      "step 1500, train accuracy 0.86\n",
      "step 1600, train accuracy 0.79\n",
      "step 1700, train accuracy 0.86\n",
      "step 1800, train accuracy 0.83\n",
      "step 1900, train accuracy 0.82\n",
      "step 2000, train accuracy 0.81\n",
      "step 2100, train accuracy 0.87\n",
      "step 2200, train accuracy 0.87\n",
      "step 2300, train accuracy 0.82\n",
      "step 2400, train accuracy 0.81\n",
      "step 2500, train accuracy 0.78\n",
      "step 2600, train accuracy 0.78\n",
      "step 2700, train accuracy 0.77\n",
      "step 2800, train accuracy 0.83\n",
      "step 2900, train accuracy 0.82\n",
      "test accuracy 0.9261\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = weight_variable('w1', [784,128])\n",
    "b1 = bias_variable('b1', [128])\n",
    "\n",
    "z1 = tf.matmul(x, w1) + b1\n",
    "a1 = tf.nn.relu(z1)\n",
    "h_fc1_drop = tf.nn.dropout(a1, keep_prob)\n",
    "\n",
    "w2 = weight_variable('w2', [128, 64])\n",
    "b2 = bias_variable('b2', [64])\n",
    "\n",
    "z2 = tf.matmul(h_fc1_drop, w2) + b2\n",
    "a2 = tf.nn.relu(z2)\n",
    "h_fc2_drop = tf.nn.dropout(a2, keep_prob)\n",
    "\n",
    "w3 = weight_variable('w3', [64, 32])\n",
    "b3 = bias_variable('b3', [32])\n",
    "\n",
    "z3 = tf.matmul(h_fc2_drop, w3) + b3\n",
    "a3 = tf.nn.relu(z3)\n",
    "h_fc3_drop = tf.nn.dropout(a3, keep_prob)\n",
    "\n",
    "w4 = weight_variable('w4', [32, 10])\n",
    "b4 = bias_variable('b4', [10])\n",
    "\n",
    "z4 = tf.matmul(h_fc3_drop, w4) + b4\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = z4))\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(z4,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(3000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.12\n",
      "step 100, train accuracy 0.94\n",
      "step 200, train accuracy 0.99\n",
      "test accuracy 0.9853\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W_conv1 = weight_variable('W_conv1', [5, 5, 1, 32])\n",
    "b_conv1 = bias_variable('b_conv1', [32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(input = x_image, filter = W_conv1, strides = [1,1,1,1], padding = 'SAME') + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(value = h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "W_conv2 = weight_variable('W_conv2', [5, 5, 32, 64])\n",
    "b_conv2 = bias_variable('b_conv2', [64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(input = h_pool1, filter = W_conv2, strides = [1,1,1,1], padding = 'SAME') + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(value = h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "W_fc1 = weight_variable('W_fc1', [7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable('b_fc1', [1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable('W_fc2', [1024, 10])\n",
    "b_fc2 = bias_variable('b_fc2', [10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(300):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.17\n",
      "step 100, train accuracy 0.96\n",
      "step 200, train accuracy 0.97\n",
      "test accuracy 0.9803\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W_conv1 = weight_variable('W_conv1', [3, 3, 1, 32])\n",
    "b_conv1 = bias_variable('b_conv1', [32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(input = x_image, filter = W_conv1, strides = [1,1,1,1], padding = 'SAME') + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(value = h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "W_conv2 = weight_variable('W_conv2', [5, 5, 32, 64])\n",
    "b_conv2 = bias_variable('b_conv2', [64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(input = h_pool1, filter = W_conv2, strides = [1,1,1,1], padding = 'SAME') + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(value = h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "W_fc1 = weight_variable('W_fc1', [7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable('b_fc1', [1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable('W_fc2', [1024, 10])\n",
    "b_fc2 = bias_variable('b_fc2', [10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(300):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.12\n",
      "step 100, train accuracy 0.98\n",
      "step 200, train accuracy 0.99\n",
      "step 300, train accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "W_conv1 = weight_variable('W_conv1', [3, 3, 1, 32])\n",
    "b_conv1 = bias_variable('b_conv1', [32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(input = x_image, filter = W_conv1, strides = [1,1,1,1], padding = 'SAME') + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(value = h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "W_conv2 = weight_variable('W_conv2', [5, 5, 32, 64])\n",
    "b_conv2 = bias_variable('b_conv2', [64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(input = h_pool1, filter = W_conv2, strides = [1,1,1,1], padding = 'SAME') + b_conv2)\n",
    "# h_pool2 = tf.nn.max_pool(value = h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "W_skconv2 = weight_variable('W_skconv2', [2, 2, 1, 64])\n",
    "b_skconv2 = bias_variable('b_skconv2', [64])\n",
    "\n",
    "x_skip1 = tf.nn.conv2d(input = x_image, filter = W_skconv2, strides = [1,2,2,1], padding = 'VALID') + b_skconv2\n",
    "\n",
    "W_conv3 = weight_variable('W_conv3', [3, 3, 64, 128])\n",
    "b_conv3 = bias_variable('b_conv3', [128])\n",
    "\n",
    "h_conv2_add = tf.add(h_conv2, x_skip1)\n",
    "\n",
    "h_conv3 = tf.nn.relu(tf.nn.conv2d(input = h_conv2_add, filter = W_conv3, strides = [1,1,1,1], padding = 'SAME') + b_conv3)\n",
    "h_pool3 = tf.nn.max_pool(value = h_conv3, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "W_fc1 = weight_variable('W_fc1', [7 * 7 * 128, 1024])\n",
    "b_fc1 = bias_variable('b_fc1', [1024])\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 7*7*128])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable('W_fc2', [1024, 10])\n",
    "b_fc2 = bias_variable('b_fc2', [10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(500):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if _%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "            print(\"step %d, train accuracy %g\"%(_, train_accuracy))\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    print(\"test accuracy %s\"% (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
